These notes are aimed at building a shared object version of GridPACK using Global Arrays built
with the simple two-sided runtime. This configuration is used in building and running the HADREC
framework. It is not optimal for running large GridPACK jobs but works fine when using HADREC,
which launches multiple single processor GridPACK tasks.

Set environment with:

module unload PrgEnv-intel
module load PrgEnv-gnu
module load boost/1.69.0
module load cmake
module load git

The user needs to build PETSc and Global Arrays in order to build GridPACK. Boost is picked up from
the boost module.

PETSc-3.7.6:

PETSc is tricky. The PETSC distribution still has a function that makes calls
the dynamic loading library. It isn't actually used anywhere but the function
is still present and causes problems with the build when GridPACK tries to
link. This requires that some configuration generated files must be edited before
compiling PETSc. Configure PETSc with

python ./config/configure.py \
  PETSC_ARCH=cori-gnu-cxx-complex-opt \
  --with-prefix=./ \
  --with-mpi=1 \
  --with-batch \
  --known-mpi-shared-libraries \
  --with-cc=cc \
  --with-fc=ftn \
  --with-cxx=CC \
  --with-c++-support=1 \
  --with-c-support=0 \
  --with-c-language=C++\
  --with-fortran=0 \
  --with-scalar-type=complex \
  --with-fortran-kernels=generic \
  --download-superlu_dist \
  --download-superlu \
  --download-parmetis \
  --download-metis \
  --download-f2cblaslapack=1 \
  --with-clanguage=c++ \
  --with-shared-libraries=1 \
  --with-x=0 \
  --with-mpirun=srun \
  --with-mpiexec=srun \
  --with-debugging=0

The "--with-batch" option causes the build to stop so that you can run part of the
configuration process on the compute nodes. The build will stop and tell you to run
the script

onftest-cori-gnu-cxx-complex-opt

by submitting it as a single process job on the compute nodes. A submission script
will look like

#!/bin/csh
#SBATCH -t 30
#SBATCH -A m3363
#SBATCH --qos=debug
#SBATCH --constraint=haswell
#SBATCH -N 1
#SBATCH -n 1
#SBATCH -o ./test.out
#SBATCH -e ./test.err

srun -n 1 conftest-cori-gnu-cxx-complex-opt

After the script runs on the compute node, a python script will appear in the build
directory.

reconfigure-cori-gnu-cxx-complex-opt.py

Run this script. After it completes, you will need to edit the
$PETSC_ARCH/include/petscconf.h that is generated by the configure process. The
following definitions need to be commented out.

PETSC_USE_SOCKET_VIEWER
PETSC_HAVE_DLFCN_H
PETSC_HAVE_DLSYM
PETSC_HAVE_DLERROR
PETSC_HAVE_DLCLOSE
PETSC_HAVE_DLOPEN

After this you can run

make all

to finish building and installing PETSc. You can also use the command generated at
the end of the PETSc configure process if you have saved it. It is essentially a
more elaborate version of 'make all'.

The process for building static libraries is nearly identical, just set the
--with-shared-libraries option to 0 when originally configuring PETSc.

GA-5-4:

These instructions should work with any recent version of Global Arrays. GA is
largely backwards compatible. The version of GA described here uses an inefficient
runtime but should not make a difference for the single process jobs used by
HADREC.

Configure ga-5-4 with the following:

../configure \
  --enable-cxx \
  --disable-f77 \
  --with-mpi \
  --enable-autodetect \
  --with-blas \
  --with-lapack \
  --with-scalapack \
  --enable-i4 \
  --enable-shared=yes \
  --enable-static=no \
  --prefix=/global/homes/b/bjpalmer/software_grid/ga-5-4/build_ts_so \
  MPICC=cc MPICXX=CC MPIF77=ftn MPIEXEX=false

This script is run in a directory build_ts_so located immediately below the
top-level GA directory. After configuring, type

make; make install

to build GA. If static libraries are desired, they can be built by switching the settings
on the --enable-shared and --enable-static options.

GridPACK:

The original attempt to build GridPACK on a Cray needed to make use of a ToolChain.cmake
file in the build directory to help CMake figure out the MPI compiler wrappers. It is
possible that this file is no longer necessary, but this method still works. The
ToolChain.cmake file looks like

# the name of the target operating system
set(CMAKE_SYSTEM_NAME Linux)

# Set search paths to prefer local, admin-installed wrappers
set(CRAY_COMPILER_SEARCH_PATHS /opt/cray/pe/craype/2.6.2/bin)

# Cray compilers
find_program(CMAKE_C_COMPILER       cc    ${CRAY_COMPILER_SEARCH_PATHS}
/opt/cray/pe/craype/2.6.2/bin/cc)
find_program(CMAKE_CXX_COMPILER     CC    ${CRAY_COMPILER_SEARCH_PATHS} 
/opt/cray/pe/craype/2.6.2/bin/CC)
# Make sure MPI_COMPILER wrapper matches the compilers.  
# Prefer local machine wrappers to driver wrappers here too.
find_program(MPI_COMPILER NAMES CC cc PATHS /opt/cray/pe/craype/2.6.2/bin)

In addition to the ToolChain.cmake file, it is also necessary to modify the
FindPackageMultipass.cmake file so that it only checks to see if test code compiles
and does not try and run it. The changes needed are to substitute the existing

include (Check${language}SourceRuns)

line with

include (Check${language}SourceCompiles)

and to change the existing

check_c_source_runs("${source}" ${testname})

check_cxx_source_runs("${source}" ${testname})

commands to

check_c_source_compiles("${source}" ${testname})

check_cxx_source_compiles("${source}" ${testname})


With the ToolChain.cmake file in the build directory and after modifying
FindPackageMultipass.cmake, GridPACK can be configured with the following script

rm -rf CMake*

CFLAGS='-L/opt/cray/xpmem/2.2.20-7.0.1.1_4.28__g0475745.ari/lib64 -lxpmem -L/opt/cray/ugni/6.0.14.0-7.0.1.1_7.63_
_ge78e5b0.ari/lib64 -lugni -L/opt/cray/udreg/2.3.2-7.0.1.1_3.61__g8175d3d.ari/lib64 -ludreg -L/opt/cray/pe/pmi/5.
0.14/lib64 -lpmi' \
      cmake -Wdev \
      -D CMAKE_TOOLCHAIN_FILE:STRING=/global/homes/b/bjpalmer/gridpack/src/build/ToolChain.cmake \
      -D PETSC_DIR:STRING='/global/homes/b/bjpalmer/software_grid/petsc-3.7.6' \
      -D PETSC_ARCH:STRING='cori-gnu-cxx-complex-opt' \
      -D GA_DIR:STRING='/global/homes/b/bjpalmer/software_grid/ga-5-4/build_ts_so' \
      -D CMAKE_INSTALL_PREFIX:PATH='/global/homes/b/bjpalmer/gridpack/src/gridpack-install' \
      -D BUILD_SHARED_LIBS:BOOL=ON \
      -D MPI_CXX_COMPILER:STRING='CC' \
      -D MPI_C_COMPILER:STRING='cc' \
      -D MPIEXEC:STRING='srun' \
      -D CHECK_COMPILATION_ONLY:BOOL=true \
      -D ENABLE_CRAY_BUILD:BOOL=true \
      -D USE_PROGRESS_RANKS:BOOL=false \
      -D CMAKE_BUILD_TYPE:STRING='RELWITHDEBINFO' \
      -D CMAKE_VERBOSE_MAKEFILE:STRING=TRUE \
      ..

The CFLAGS variable must be set to include a large number of libraries needed by MPI. If
building with static libraries, then the script can be modified to

rm -rf CMake*

export CRAYPE_LINK_TYPE=static

cmake -Wdev \
      -D CMAKE_TOOLCHAIN_FILE:STRING=/global/homes/b/bjpalmer/gridpack/src/build/ToolChain.cmake \
      -D PETSC_DIR:STRING='/global/homes/b/bjpalmer/software_grid/petsc-3.7.6' \
      -D PETSC_ARCH:STRING='cori-gnu-cxx-complex-opt-static' \
      -D GA_DIR:STRING='/global/homes/b/bjpalmer/software_grid/ga-5-4/build_ts' \
      -D CMAKE_INSTALL_PREFIX:PATH='/global/homes/b/bjpalmer/gridpack/src/gridpack-install' \
      -D BUILD_SHARED_LIBS:BOOL=OFF \
      -D MPI_CXX_COMPILER:STRING='CC' \
      -D MPI_C_COMPILER:STRING='cc' \
      -D MPIEXEC:STRING='srun' \
      -D CHECK_COMPILATION_ONLY:BOOL=true \
      -D ENABLE_CRAY_BUILD:BOOL=true \
      -D USE_PROGRESS_RANKS:BOOL=false \
      -D CMAKE_BUILD_TYPE:STRING='RELWITHDEBINFO' \
      -D CMAKE_VERBOSE_MAKEFILE:STRING=TRUE \
      -D CMAKE_EXE_LINKER_FLAGS:STRING='-Wl,-zmuldefs -Wl,-y_dl_x86_cpu_features' \
      ..

It is no longer necessary to include multiple libraries in the CFLAGS parameter.
